\documentclass[]{report}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}

\usepackage{zymacros}

%%%%%%%%%%%%%%%%%%%%%%%
% \NewDocumentCommand{\tens}{t_}
%  {%
%   \IfBooleanTF{#1}
%   {\tensop}
%   {\otimes}%
%  }
% \NewDocumentCommand{\tensop}{m}
%  {%
%   \mathbin{\mathop{\otimes}\displaylimits_{#1}}%
%  }

%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Standard notation suggestion for Machine learning}
\author{}

\maketitle 
%\date{\today}
\begin{abstract}
This introduces a suggestion of mathematical notation protocol for machine learning.
\end{abstract}

\paragraph{Dataset}~\\
Dataset  $S=\{\vz_i\}_{i=1}^n=\{(\vx_i,\vy_i)\}_{i=1}^n$ is sampled from a distribution $\fD$ over a domain $\fZ=\fX\times\fY$. 

$\fX$  is the instances domain (a set), $\fY$ is the label domain (a set), and $\fZ=\fX\times\fY$ is the examples domain (a set). 

Usually, 
$\fX$ is a subset of $\sR^d$ and $\fY$ is a subset of $\sR^{d_{o}}$, where $d$ is the input dimension, $d_{o}$ is the output dimension.

$n=\#S$ is the number of samples. Without other specified, $S$ and $n$ are for the training set. The test set can be denoted by $S_{\rm test}$ and $n_{\rm test}$, or $S_{\rm t}$ and $n_{\rm t}$.

\paragraph{Function}~\\
Hypothesis space is denoted by $\fH$. Hypothesis function is denoted by $f_{\theta}\in\fH$ with $f_{\theta}:\fX\to\fY$.

$\vtheta$  denotes the set of parameters of  $f_{\theta}$.  

The target function is denoted by $f^*$or $f:\fX\to\fY$ satisfying $\vy_i=f^*(\vx_i)$ for $i=1,\ldots,n$.

\paragraph{Loss function}~\\
Loss function, denoted by $\ell:\fY\times\fY\to\sR_+:=[0,+\infty)$, measures the difference between a predicted label and a true label, e.g. $L^2$ loss: $\ell(f_{\theta}(\vx),\vy)=(f_{\theta}(\vx)-\vy)^2$.

Empirical risk or training loss is denoted by   
\begin{equation}
    \LS(\vtheta) =\frac{1}{n}\sum_{i=1}^n\ell(f_{\theta}(\vx_i),\vy_i),
\end{equation}

The population risk or expected loss is denoted by
\begin{equation}
    \LD(\vtheta) =\Exp_{\fD}\ell(f_{\theta}(\vx),\vy).
\end{equation}
The test loss is denoted by
\begin{equation}
    L_{S_{\rm t}}(\vtheta) =\frac{1}{n_{\rm t}}\sum_{(\vx,\vy)\in S_{\rm t}}\ell(f_{\theta}(\vx),\vy).
\end{equation}
Without further explanation, $L$ will be used for $L_S$.

\paragraph{Accuracy}~\\
The accuracy is denoted by $\mu$ or $\mu_S$ NEED TO CHECK
\begin{equation}
    \mu =\frac{1}{n}\sum_{(\vx,\vy)\in S}\sone_{f_{\theta}(\vx)=\vy},
\end{equation}

The expected accuracy is denoted by $\mu_{\fD}$  NEED TO CHECK
\begin{equation}
    \mu_{\fD} =\Exp_{\fD}\sone_{f_{\theta}(\vx)=\vy},
\end{equation}

\paragraph{Activation function}
Activation function is denoted by $\sigma(x)$. 
\begin{exam}Some commonly used activation functions are~\\
    \begin{enumerate}
        \item $\sigma(z) = \ReLU (z) = \max (0, z) = z^+$;
        \item $\sigma(z) ={\rm sigmoid}(z)= \frac{1}{1 + \E^{-z}}$;
        \item $\sigma(z) = \tanh z$;
        \item $\sigma(z) = \cos z, \sin z$.
    \end{enumerate}
\end{exam}


\paragraph{Two-layer neural network}~\\
The neuron number of the hidden layer is denoted by $m$.
\begin{equation}
    f_{\theta}(\vx)=\sum_{j=1}^{m} a_j \sigma (\vw_j\cdot \vx + b_j),
\end{equation}
where $\sigma$ is the activation function, $\vw_j$ is the input weight, $a_j$ is the output weight, $b_j$ is the bias term.
\paragraph{General deep neural network}~\\
The counting of the layer number excludes the input layer. The $(H+1)$-layer neural network is denoted by
\begin{equation}
    h(\vx,\vtheta) = \vW^{[H]} \sigma\circ(\mW^{[H-1]}\sigma\circ(\cdots (\mW^{[1]} \sigma\circ(\mW^{[0]} \vx + \vb^{[0]} ) + \vb^{[1]} )\cdots)+\vb^{[H-1]})+\vb^{[H]},
\end{equation}
where $\mW^{[l]} \in \sR^{m_{l}\times m_{l-1}}$, $\vb^{[l]}=\sR^{m_{l}}$, $m_0=d_{\rm in}=d$, $m_L=d_{\rm out}$,
$\sigma$ is a scalar function and ``$\circ$'' means entry-wise operation. 
We denote $\vtheta=(\mW^{[0]},\mW^{[1]},\ldots,\vW^{[H-1]},\vb^{[0]},\vb^{[1]},\ldots,\vb^{[H-1]})$. This can also be defined recursively.
\begin{align}
    &f^{[0]}(\vx)=\vx, \\
    &f^{[l]}(\vx)=\sigma\circ(\mW^{[l-1]} h^{[l-1]}(\vx) + \vb^{[l-1]}) \quad 1\leq l\leq H-1,\\
    &f_{\vtheta}(\vx)=h^{[H]}(\vx)=\mW^{[H-1]} h^{[H-1]}(\vx) + \vb^{[H-1]}.
\end{align}

\paragraph{Fourier Frequency}~\\
The discretized frequency is denoted by $\vk$, and the continuous frequency is denoted by $\vxi$.
\paragraph{Complexity}~\\
The VC-dimension of a hypothesis class $\fH$ is denoted ${\rm VCdim}(\fH)$.

The Rademacher complexity of a hypothesis space $\fH$ on a sample set $S$ is denoted by $R (\fH\circ S)$.

\paragraph{Training}~\\
The Gradient Descent is often denoted by GD. The Stochastic Gradient Descent is often denoted by SGD. 

The learning rate is denoted by $\eta$.

\paragraph{Gram matrix}~\\
The Gram matrix is denoted by $K_n$.

\paragraph{More notations}~\\
More notations about GAN, RNN, CNN, Resnet etc. are left to be done.


\end{document}
