\documentclass[]{report}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}

\usepackage{listings}
\lstset
{
    language=[LaTeX]TeX,
    breaklines=true,
    basicstyle=\tt\scriptsize,
    keywordstyle=\color{blue},
    identifierstyle=\color{magenta},
}

\usepackage{zymacros}

%%%%%%%%%%%%%%%%%%%%%%%
% \NewDocumentCommand{\tens}{t_}
%  {%
%   \IfBooleanTF{#1}
%   {\tensop}
%   {\otimes}%
%  }
% \NewDocumentCommand{\tensop}{m}
%  {%
%   \mathbin{\mathop{\otimes}\displaylimits_{#1}}%
%  }

%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Mathematical notation protocol for machine learning}
\author{}
\date{\today}
%\begin{abstract}
%\end{abstract}
\maketitle
\begin{abstract}
    This introduces a suggestion of mathematical notation protocol for machine learning.
\end{abstract}

\section{General conceptions}
\begin{center}
    \begin{tabular}{l|l|l|l}
        \hline
        symbol & meaning & \LaTeX & simplied\\
        \hline

        $\vx$ & input & \verb!\bm{x}! & \verb!\vx! \\
        $\vy$ & output, label & \verb!\bm{y}! & \verb!\vy! \\
        $d$ & input dimension & \verb!d! &  \\
        $d_{\rm o}$ & output dimension &\verb!d_{\rm o}! &  \\
        $n$ & number of samples & \verb!n!  \\
        $\fX$
        & instances domain (a set)&\verb!\mathcal{X}!&\verb!\fX!\\
        $\fY$
        & labels domain (a set)&\verb!\mathcal{Y}!&\verb!\fY!\\
        $\fZ$
        & $=\fX\times\fY$ examples domain (a set)&\verb!\mathcal{Z}!&\verb!\fZ!\\
        $\fH$
        & hypothesis space (a set)&\verb!\mathcal{H}!&\verb!\fH!\\
        $\vtheta$ & a set of parameters & \verb!\bm{\theta}! &\verb!\vtheta!\\
        $f_{\vtheta}:\fX\to \fY$ & hypothesis function with parameters $\vtheta$ & \verb!\f_{\bm{\theta}}! & \verb!f_{\vtheta}! \\
        $f$ or $f^*:\fX\to\fY$ & target function  & \verb!f,f^*!   \\
        $\ell:\fH\times \fZ\to \sR^+$ & loss function & \verb!\ell! \\
        $\fD$
        & a distribution over some set (usually $\fZ$)&\verb!\mathcal{D}!&\verb!\fD!\\

        %$z\sim\fD$
%        & sampling $z$ according to $\fD$\\
        $S=\{\vz_i\}_{i=1}^n$
        & $=\{(\vx_i,\vy_i)\}_{i=1}^n$ is sampled from  $\fD$ over  $\fZ$\\
        %$S\sim\fD^n$
%        & sampling $S=z_1,\ldots,z_n$ i.i.d. according to $\fD$\\
        $\LS(\vtheta)$ or $\hat{L}_{n}(\vtheta)$ & $=\sum_{i=1}^n\ell(f_{\vtheta},\vz_i)$ empirical risk or training loss\\
        %&\verb!\L_S(\bm{\theta}),\hat{L}_n(\bm{\theta})!&\verb!\LS(\vtheta),\hat{L}_n(\vtheta)!\\
        $\LD(\vtheta)$ &$=\Exp_{\vz\sim\fD}\ell(f_{\vtheta},\vz)$ population risk or expected loss\\
%        & \verb!\L_\mathcal{D}(\bm{\theta})!&\verb!\LD(\vtheta)!\\
        $\sigma:\sR\to\sR^+$& activation function &\verb!\sigma!\\
        $\vw_j$ &input weight&\verb!\bm{w}_j!&\verb!\vw_j!\\
        $a_j$ &output weight &\verb!a_j!\\
        $b_j$ &bias term &\verb!b_j!\\
        $f_{\vtheta}(\vx)$&$=\sum_{j=1}^{m} a_j \sigma (\vw_j\cdot \vx + b_j) $ two-layer neural network &\verb!f_{\bm{\theta}}!&\verb!f_{\vtheta}!\\
         ${\rm VCdim}(\fH)$& the VC-dimension of a hypothesis space $\fH$ \\
%         &\verb!{\rm VCdim}(\mathcal{H})!&\verb!{\rm VCdim}(\fH)!
        $R (\fH\circ S)$& the Rademacher complexity of $\fH$ on $S$\\
        GD& gradient descent\\
        SGD &stochastic gradient descent\\
        $\eta$&learning rate&\verb!\eta!\\
        $K_n$&Gram matrix&\verb!K_n!\\
        $\vk$&discretized frequency&\verb!\bm{k}!&\verb!\vk!\\
        $\vxi$&continuous frequency&\verb!\bm{\xi}!&\verb!\vxi!\\
        $*$&convolution operation &\verb!*!\\
    \end{tabular}
\end{center}
\newpage
\section{$(H+1)$-layer neural network }
\begin{center}
    \begin{tabular}{l|l|l|l}
        \hline
        symbol & meaning & \LaTeX & simplied\\
        \hline
        $d$ & input dimension & \verb!d! &  \\
        $d_{\rm o}$ & output dimension &\verb!d_{\rm o}! &  \\
        $m_l$& the number of $l$-1th hidden layer neuron, $m_0=d$, $m_{H+1} = d_{\rm o}$&\verb!m_l!\\
        $\mW^{[l]}$ & the $l$-1th layer weight &\verb!\bm{W}^{[l]}!&\verb!\mW^{[l]}!\\
        $\vb^{[l]}$ & the $l$-1th layer bias term&\verb!\bm{b}^{[l]}!&\verb!\vb^{[l]}!\\
        $\circ$&entry-wise operation&\verb!\circ!\\
        $\sigma:\sR\to\sR^+$& activation function &\verb!\sigma!\\
        $\vtheta$&$=(\mW^{[0]},\mW^{[1]},\ldots,\vW^{[H]},\vb^{[0]},\vb^{[1]},\ldots,\vb^{[H]})$, a set of parameters&\verb!\bm{\theta}!&\verb!\vtheta!\\
        $f_{\vtheta}^{[0]}(\vx)$&$=\vx$\\
        $f_{\vtheta}^{[l]}(\vx)$&$=\sigma\circ(\mW^{[l-1]} f_{\vtheta}^{[l-1]}(\vx) + \vb^{[l-1]})$,  $l$-th hidden layer input \\
         $f_{\vtheta}(\vx)$&$=f_{\vtheta}^{[H+1]}(\vx)=\mW^{[H]} f_{\vtheta}^{[H]}(\vx) + \vb^{[H]}$,  $(H+1)$-layer neural network\\
    
    \end{tabular}
\end{center}
\end{document}
